# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: Import ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
# =============================================================================
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import time
import os

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
# =============================================================================
class BigDataTimeSeriesDataset(Dataset):
    """
    ‡∏Ñ‡∏•‡∏≤‡∏™ Dataset ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
    ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô (chunk) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
    """
    def __init__(self, csv_path, chunk_size, sequence_length, input_cols, output_col):
        """
        Constructor ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™
        
        ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå:
        - csv_path (str): ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
        - chunk_size (int): ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        - sequence_length (int): ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏ô‡∏∏‡∏Å‡∏£‡∏°‡πÄ‡∏ß‡∏•‡∏≤ (window size) ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô input
        - input_cols (list): ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Input (‡πÄ‡∏ä‡πà‡∏ô ['DATA_INPUT', 'DATA_OUTPUT'])
        - output_col (str): ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Output (‡πÄ‡∏ä‡πà‡∏ô 'DATA_OUTPUT')
        """
        self.csv_path = csv_path
        self.chunk_size = chunk_size
        self.sequence_length = sequence_length
        self.input_cols = input_cols
        self.output_col = output_col

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Scaler ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Input ‡πÅ‡∏•‡∏∞ Output ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        self.scaler_input = MinMaxScaler()
        self.scaler_output = MinMaxScaler()

        print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏™‡πÄ‡∏Å‡∏• (Fitting Scaler)...")
        self._fit_scalers()
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á Dataset
        # ‡πÄ‡∏£‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô __len__
        print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå...")
        self.total_rows = sum(1 for row in open(self.csv_path)) - 1 # -1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö header
        print(f"‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {self.total_rows} ‡πÅ‡∏ñ‡∏ß")

        self.sequences = []
        self.labels = []
        
    def _fit_scalers(self):
        """
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏≠‡∏ô (fit) Scaler ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏ö‡∏ö chunk
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ min/max ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤ RAM
        """
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á TextFileReader ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞ chunk
        reader = pd.read_csv(self.csv_path, chunksize=self.chunk_size, iterator=True)
        
        start_time = time.time()
        for i, chunk in enumerate(reader):
            # partial_fit ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡πà‡∏≤ min/max ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô
            self.scaler_input.partial_fit(chunk[self.input_cols])
            self.scaler_output.partial_fit(chunk[[self.output_col]]) # ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡πÉ‡∏ô list 2 ‡∏ä‡∏±‡πâ‡∏ô
            if (i + 1) % 10 == 0:
                 print(f"  - Fit Scaler ‡∏à‡∏≤‡∏Å Chunk ‡∏ó‡∏µ‡πà {i+1}...")
        
        end_time = time.time()
        print(f"‡∏õ‡∏£‡∏±‡∏ö‡∏™‡πÄ‡∏Å‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡πÉ‡∏ô {end_time - start_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")

    def __len__(self):
        """
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÉ‡∏ô Dataset class
        ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô sample ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ
        """
        return self.total_rows - self.sequence_length

    def __getitem__(self, idx):
        """
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!
        ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 1 sample (sequence ‡πÅ‡∏•‡∏∞ label) ‡∏ì ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (index) ‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≠‡∏á‡∏Ç‡∏≠
        
        ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå:
        - idx (int): ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á sample ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        """
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV
        start_row = idx
        end_row = idx + self.sequence_length + 1 # +1 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ label ‡∏î‡πâ‡∏ß‡∏¢
        
        # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV
        # skiprows ‡∏à‡∏∞‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î, nrows ‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥!
        df_slice = pd.read_csv(
            self.csv_path,
            skiprows=range(1, start_row + 1), # +1 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ skiprows ‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö header
            nrows=self.sequence_length + 1,
            header=None, # ‡πÑ‡∏°‡πà‡∏°‡∏µ header ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß
            names=['DATA_INPUT', 'DATA_OUTPUT', 'TIME'] # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏≠‡∏á
        )
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Input ‡πÅ‡∏•‡∏∞ Output
        input_data = df_slice[self.input_cols].values
        output_data = df_slice[[self.output_col]].values
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏™‡πÄ‡∏Å‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ Scaler ‡∏ó‡∏µ‡πà fit ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        scaled_input = self.scaler_input.transform(input_data)
        scaled_output = self.scaler_output.transform(output_data)

        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô sequence (X) ‡πÅ‡∏•‡∏∞ label (y)
        sequence = scaled_input[:self.sequence_length]
        label = scaled_output[self.sequence_length]

        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô PyTorch Tensors
        return torch.FloatTensor(sequence), torch.FloatTensor(label)

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á LSTM Model)
# =============================================================================
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á hidden state ‡πÅ‡∏•‡∏∞ cell state ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ LSTM
        out, _ = self.lstm(x, (h0, c0))
        
        # ‡πÉ‡∏ä‡πâ output ‡∏à‡∏≤‡∏Å timestep ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        out = self.fc(out[:, -1, :])
        return out

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏´‡∏•‡∏±‡∏Å (Main Execution)
# =============================================================================
if __name__ == '__main__':
    # --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Hyperparameters ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÜ ---
    CSV_FILE_PATH = 'data_log_simulation_600K.csv'  # <-- ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô path ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå
    CHUNKSIZE = 50000      # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î RAM)
    SEQUENCE_LENGTH = 30   # ‡∏Ç‡∏ô‡∏≤‡∏î Window size
    INPUT_COLUMNS = ['DATA_INPUT', 'DATA_OUTPUT']
    OUTPUT_COLUMN = 'DATA_OUTPUT'
    
    BATCH_SIZE = 128       # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Batch (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î VRAM ‡∏Ç‡∏≠‡∏á GPU)
    EPOCHS = 5
    LEARNING_RATE = 0.001
    
    # --- 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ GPU ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå (Device) ---
    # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Jetson Nano!
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üöÄ ‡πÉ‡∏ä‡πâ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå: {device}")

    # --- 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Instance ‡∏Ç‡∏≠‡∏á Dataset ‡πÅ‡∏•‡∏∞ DataLoader ---
    print("\n--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ---")
    full_dataset = BigDataTimeSeriesDataset(
        csv_path=CSV_FILE_PATH,
        chunk_size=CHUNKSIZE,
        sequence_length=SEQUENCE_LENGTH,
        input_cols=INPUT_COLUMNS,
        output_col=OUTPUT_COLUMN
    )

    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ 80/20)
    train_size = int(0.8 * len(full_dataset))
    test_size = len(full_dataset) - train_size
    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])
    
    print(f"‡∏Ç‡∏ô‡∏≤‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Train: {len(train_dataset)}")
    print(f"‡∏Ç‡∏ô‡∏≤‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Test: {len(test_dataset)}")
    
    print("\n--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ---")
    # DataLoader ‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Dataset ‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô Batch ‡πÉ‡∏´‡πâ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    # num_workers > 0 ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ CPU core ‡∏ä‡πà‡∏ß‡∏¢‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ó‡∏≥‡πÉ‡∏´‡πâ GPU ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠ (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å)
    train_loader = DataLoader(
        dataset=train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, # ‡∏™‡∏•‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ epoch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
        num_workers=2 # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô core ‡∏Ç‡∏≠‡∏á CPU
    )
    test_loader = DataLoader(
        dataset=test_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=False,
        num_workers=2
    )
    
    # --- 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•, Loss Function ‡πÅ‡∏•‡∏∞ Optimizer ---
    model = LSTMModel(input_size=len(INPUT_COLUMNS), hidden_size=50, num_layers=1, output_size=1).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # --- 5. ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ---
    print("\n--- üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ---")
    total_training_time = 0
    
    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train() # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏ó‡∏£‡∏ô
        
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞ Batch ‡∏à‡∏≤‡∏Å DataLoader
        # tqdm ‡πÄ‡∏õ‡πá‡∏ô library ‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô progress bar ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏° (‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢ pip install tqdm)
        from tqdm import tqdm
        
        for i, (sequences, labels) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")):
            # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Batch ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà Device (GPU/CPU)
            sequences = sequences.to(device)
            labels = labels.to(device)

            # Forward pass
            outputs = model(sequences)
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        total_training_time += epoch_duration
        
        print(f"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.6f}, Time: {epoch_duration:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")

    print(f"\n--- ‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ---")
    print(f"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_training_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
    
    # --- 6. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• (Evaluation) ---
    print("\n--- üìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏• ---")
    model.eval() # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡πÇ‡∏´‡∏°‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
    all_predictions = []
    all_actuals = []
    
    with torch.no_grad(): # ‡πÑ‡∏°‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì gradient ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
        for sequences, labels in tqdm(test_loader, desc="Evaluating"):
            sequences = sequences.to(device)
            labels = labels.to(device)
            
            outputs = model(sequences)
            
            # ‡∏¢‡πâ‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏µ‡πà CPU ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô NumPy array ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
            all_predictions.append(outputs.cpu().numpy())
            all_actuals.append(labels.cpu().numpy())

    # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Batch ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
    predictions_flat = np.concatenate(all_predictions).flatten()
    actuals_flat = np.concatenate(all_actuals).flatten()

    # **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å:** ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πÄ‡∏Å‡∏•‡πÄ‡∏î‡∏¥‡∏°
    # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á reshape ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 2D array ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ inverse_transform
    predictions_inversed = full_dataset.scaler_output.inverse_transform(predictions_flat.reshape(-1, 1))
    actuals_inversed = full_dataset.scaler_output.inverse_transform(actuals_flat.reshape(-1, 1))
    
    from sklearn.metrics import mean_squared_error
    mse = mean_squared_error(actuals_inversed, predictions_inversed)
    print(f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Test Set):")
    print(f"  - Mean Squared Error (MSE): {mse:.4f}")
    print(f"  - Root Mean Squared Error (RMSE): {np.sqrt(mse):.4f}")